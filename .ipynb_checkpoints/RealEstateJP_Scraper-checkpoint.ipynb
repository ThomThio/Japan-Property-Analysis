{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ODD import ODD as ODD\n",
    "import traceback\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cities(cities, page_lim=1):\n",
    "\tdfs = []\n",
    "\tfor city in cities:\n",
    "\t\tcity_urls = []\n",
    "\t\tfor page in range(page_lim):\n",
    "\t\t\tdistrict_link = \"https://realestate.co.jp/forsale/listing?prefecture=JP-13&city=\" + str(city) + \"-ku&district=&max_price=&search=Search&page=\" + str(page)\n",
    "\n",
    "\t\t\t#go to link, grab all the links of the listings, go to next page\n",
    "\t\t\tres = requests.get(district_link)\n",
    "\t\t\tsoup = BeautifulSoup(res.content,'lxml')\n",
    "\t\t\t# listings = soup.find_all('div',attrs={'class','property-listing'}) #<div class=\"property-listing\"\n",
    "\t\t\tlistings = [a.get('href') for a in soup.find_all('a',href=True)]\n",
    "\t\t\tlistings = list(set(listings))\n",
    "\t\t\tcleaned_urls = []\n",
    "\t\t\tfor l in listings:\n",
    "\t\t\t\tif 'view' in l:\n",
    "\t\t\t\t\tcity_urls.append(l)\n",
    "\n",
    "\t\tdf = pd.DataFrame(city_urls,columns=[city])\n",
    "\t\tdfs.append(df)\n",
    "\n",
    "\tcombined = pd.concat(dfs,axis=1)\n",
    "\tprint(combined.head(20))\n",
    "\tcombined.to_excel(\"Urls.xlsx\")\n",
    "\treturn combined\n",
    "\t\t#compile these, save it, iterate through it after\n",
    "\n",
    "\n",
    "\n",
    "#read list of links, auto scrape accordingly here.\n",
    "def retrieve_from_scraped_res(storage_dict,table):\n",
    "\tlast_key = \"\"\n",
    "\tfor k in table:\n",
    "\n",
    "\t\tif '<dt' in str(k):\n",
    "\t\t\tstart_ix = str(k).find('>') + 1\n",
    "\t\t\tend_ix = str(k).rfind('<')\n",
    "\t\t\ttemp = str(k)[start_ix:end_ix]\n",
    "\t\t\tlast_key = temp.strip('\\n\\r\\t\": ')\n",
    "\t\t\tstorage_dict[last_key] = None\n",
    "\t\telif '<dd' in str(k):\n",
    "\t\t\tstart_ix = str(k).find('>') + 1\n",
    "\t\t\tend_ix = str(k).rfind('<')\n",
    "\t\t\ttemp_val = str(k)[start_ix:end_ix]\n",
    "\t\t\ttemp_val = temp_val.replace(\"<br/>\",\",\")\n",
    "\t\t\tif 'Nearest Station' in last_key:\n",
    "\t\t\t\tspl = temp_val.split(\",\")[0]\n",
    "\t\t\t\ttime_to_station = spl[spl.find(\"(\")+1:-1]\n",
    "\t\t\t\tstorage_dict[\"Nearest Station\"] = spl[:spl.find(\"(\")].strip('\\n\\r\\t\": ')\n",
    "\t\t\t\tstorage_dict[\"WalkingTime\"] = time_to_station.strip('\\n\\r\\t\": ')\n",
    "\t\t\t\tstorage_dict[\"Station Line\"] = temp_val.split(\",\")[1].strip('\\n\\r\\t\": ')\n",
    "\t\t\telse:\n",
    "\t\t\t\tstorage_dict[last_key] = temp_val.strip('\\n\\r\\t\": ')\n",
    "\t\t\t\t\t\n",
    "\t\t\t# print(last_key,\":\",temp_val)\n",
    "\treturn storage_dict\n",
    "\n",
    "\n",
    "def execute(link):\n",
    "\tprint(link)\n",
    "\tres = requests.get(link)\n",
    "\tsoup = BeautifulSoup(res.content,'lxml')\n",
    "\n",
    "\n",
    "\ttables = soup.find_all('dl')\n",
    "\t# print(tables)\n",
    "\n",
    "\ttable1 = tables[0]\n",
    "\ttable2 = tables[1]\n",
    "\n",
    "\tstorage_dict = ODD()\n",
    "\n",
    "\n",
    "\tstorage_dict = retrieve_from_scraped_res(storage_dict,table1)\n",
    "\tstorage_dict = retrieve_from_scraped_res(storage_dict,table2)\n",
    "\n",
    "\t#turn dictionary in dataframe\n",
    "\t#compile dataframes\n",
    "\tdf = pd.DataFrame.from_dict(storage_dict,orient='index')\n",
    "\n",
    "\treturn df\n",
    "\n",
    "\n",
    "\n",
    "def start_crawl(urls):\n",
    "\tcompiled_dfs = []\n",
    "\tfor i in urls:\n",
    "\t\tprint(i)\n",
    "\t\tfor l in urls[i]:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdf = execute(\"https://realestate.co.jp\" +l)\n",
    "\t\t\t\t# df = execute(l)\n",
    "\t\t\t\tcompiled_dfs.append(df)\n",
    "\t\t\texcept:\n",
    "\t\t\t\ttraceback.print_exc()\n",
    "\t\t\t\tprint(\"Saw:\",l)\n",
    "\n",
    "\t\ttime.sleep(2)\n",
    "\n",
    "\tmaster_df = pd.concat(compiled_dfs,axis=1)\n",
    "\tmaster_df = master_df.transpose()\n",
    "\tprint(master_df.head())\n",
    "\tmaster_df.to_excel(\"Listings.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_listings(crawled_listings):\n",
    "\t#replace nans with 0\n",
    "\tcrawled_listings.fillna(0,inplace=True)\n",
    "\n",
    "\t#prices\n",
    "\tcrawled_listings['Maintenance Fee'] = crawled_listings['Maintenance Fee'].apply(lambda x: str(x).split(\" \")[0].replace(\",\",\"\")[1:])\n",
    "\tcrawled_listings['Price'] = crawled_listings['Price'].apply(lambda x: str(x).split(\" \")[0].replace(\",\",\"\")[1:]).astype(float)\n",
    "\n",
    "\t#sizes\n",
    "\tcrawled_listings['Balcony Size'] = crawled_listings['Balcony Size'].apply(lambda x: str(x).split(\" \")[0]).astype(float) #m2\t\n",
    "\tcrawled_listings['Size'] = crawled_listings['Size'].apply(lambda x: str(x).split(\" \")[0].replace(',','')).astype(float) #m2\t\n",
    "\n",
    "\t#time\n",
    "\tcrawled_listings['WalkingTime'] = crawled_listings['WalkingTime'].apply(lambda x: str(x).split(\" \")[0]).astype(int) #mins\t\n",
    "\n",
    "\n",
    "\n",
    "\t# print(crawled_listings['Size'])\n",
    "\treturn crawled_listings\n",
    "\n",
    "def generate_features(listings):\n",
    "\t#get current year\n",
    "\timport datetime\n",
    "\tyear = 2020\n",
    "\tlistings['Age'] = year - listings['Year Built']\n",
    "\n",
    "\tlistings['FloorLevel'] = listings['Floor'].apply(lambda x: str(x).split(\"/\")[0].strip())\n",
    "\n",
    "\tlistings['Price Psm'] = listings['Price']/listings['Size']\n",
    "\n",
    "\tltv = 0.8\n",
    "\tsgdjpy = 80\n",
    "\tlistings['Upfront PMT (SGD)'] = listings['Price'] * (1-ltv) / sgdjpy\n",
    "\n",
    "\trate = 0.019\n",
    "\tpmt_years = 30\n",
    "\tlistings['Monthly PMT (SGD)'] = (listings['Price'] * ltv)  * (1+rate) / 30 / 12 / sgdjpy\n",
    "\n",
    "\n",
    "\n",
    "\t# print(listings[['Age','FloorLevel']])\n",
    "\treturn listings\n",
    "\n",
    "\n",
    "#filter for critieria\n",
    "def filter(crawled_listings, cities=None):\n",
    "\tcrawled_listings = crawled_listings[crawled_listings['Age'] <= 40]\n",
    "\tcrawled_listings = crawled_listings[crawled_listings['Size'] >= 25]\n",
    "\tcrawled_listings = crawled_listings[crawled_listings['Land Rights'] == \"Freehold\"]\n",
    "\t# crawled_listings = crawled_listings[crawled_listings['Station Line'].isin(['JR Yamanote Line','Tokyo Metro Hibiya Line','Tokyo Metro Line'])]\n",
    "\tcrawled_listings = crawled_listings[crawled_listings['WalkingTime'] <= 7]\n",
    "\n",
    "\t#rank order by price psm ascending (cheapest)\n",
    "\tcrawled_listings.sort_values(by=['Upfront PMT (SGD)','Price Psm'],inplace=True)\n",
    "\n",
    "\tpresentation_cols = ['Price Psm','Location','Building Name','Layout','Nearest Station','Station Line','WalkingTime','Upfront PMT (SGD)','Monthly PMT (SGD)','Price','Size','Age','Type','Maintenance Fee','Balcony Size','Direction Facing','FloorLevel','Gross Yield','Occupancy','Other Expenses']\n",
    "\toutput = crawled_listings[presentation_cols]\n",
    "\n",
    "\t#filter for all those in specific cities, if given\n",
    "\tif cities is not None:\n",
    "\t\toutput = output[output['Location'].isin(cities)]\n",
    "\n",
    "\toutput.to_excel(\"Output.xlsx\")\n",
    "\n",
    "\tprint(output.head(20))\n",
    "\n",
    "\n",
    "\treturn crawled_listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities =['shibuya','taito','bunkyo','sumida','shinagawa','meguro','ota','setagaya','nakano','suginami','toshima','kita','arakawa','itabashi','nerima','adachi','katsushika','edogawa','']\n",
    "urls = scrape_cities(cities,20) #5 mins taken, all cities, 20 pages each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = pd.read_excel('Urls.xlsx')\n",
    "# print(len(urls) * len(cities),\"links extracted\")\n",
    "# start_crawl(urls) #[Finished in 4373.7s] #7.3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_listings = pd.read_excel(\"Listings.xlsx\")\n",
    "crawled_listings = clean_listings(crawled_listings)\n",
    "crawled_listings = generate_features(crawled_listings)\n",
    "crawled_listings.to_excel(\"Output.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter(crawled_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
